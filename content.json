{"posts":[{"title":"Kubernetes与Nacos同步ConfigMap方案","text":"Kubernetes与Nacos同步ConfigMap方案 背景Nacos配置中心正在和K8S的ConfigMap打通，未来需要实现对于K8S数据的管控、审批、变更通知功能。 社区Issue支持从K8S的ConfigMap中，实时同步配置数据到Nacos的配置中心 同步方式方案选择——两种方案对比同步任务放到Nacos主进程中在Nacos主模块中新增两个事件监听器: 第一个：当有配置变更事件时，触发一个通知，发布至K8S的ConfigMap； 第二个：监听K8S的配置变化事件，有变化时发布至Nacos的配置中心。 发起数据变更 定期对账 单独开启一个进程负责同步任务 单独进程负责监听K8S侧的数据，并发布到Nacos的配置中心中； 监听Nacos的配置变化事件，有变化时发布到K8S上。 发起数据变更 定期对账 两种方案对比 方式\\比较点 用户使用成本 后期接入配置变更审批功能 社区关注程度 Nacos更加臃肿 Nacos主进程中 一键部署 方便，直接集成在nacos的主进程控制台中 用户会看到Nacos主进程中出现了K8S模块，运营宣传成本低 单独进程负责同步 还需要再nacos-sync的那个管控界面加入审批功能 如果新增一个项目在nacos-group下，绝大多数开源用户不会关注，需要额外投入运营成本 下述描述的是第一种同步方式的实现方案。 哪台节点负责这项监听+写入的工作？——三种写入方式对比Nacos是以集群部署的，需要明确监听K8S侧数据+拉取+写入DB是哪台节点的责任。以下提供三种划分责任节点的方式。 方式一：选择一台机器来监听K8S侧，并写入DB 对所有健康节点的address进行排序，排名第一的节点负责去监听、并同步ConfigMap数据，写入DB； 当发生节点列表变更（MembersChangeEvent事件）时，重新排序。 优点：省性能。缺点： 如果每台机器上的机器列表排序不一致，则会出现责任节点不明确的问题；需要额外限定一种统一的排序方式； 如果发生某台节点的频繁上下线，会发生责任节点的频繁转移。 方式二：冗余写入–每台Nacos机器都负责监听K8S侧+写入DB该方式认为：监听ConfigMap是每一台机器的责任，当ConfigMap上的数据发生变化的时候，每一台机器都要去同步一遍。 优点：能够保证监听任务的冗余备份，一台挂掉，其他机器还可以工作，且没有某台机器频繁上下线引入的责任变更问题。 方式三：用DistroMapper来划分写入范围，每台节点负责一部分配置 缺点：Config复用了Distro的代码，依赖关系会变复杂。 ConfigMap到Nacos的数据结构映射关系从kubernetes-&gt;nacos中的configMap同步，两者相关字段的映射关系如下： nacos字段 值 data_id ${kubernetes.configMap.metadata.name} group_id K8S_GROUP centent ${kubernetes.configMap完整yaml} md5 自动生成 gmt_create 当前时间 gmt_modified 当前时间 src_user nacos src_ip ${kubernetes api server ip} tenant_id ${kubernetes.configMap.metadata.namespace} type yaml 开关设计 系统属性-总开关：nacos.k8s.configMap.enable为true，默认为false 系统属性-是否责任节点：当前节点如果被选择为负责kubernetes-&gt;nacos的configMap的同步节点，则写入一个nacos.k8s.configMap.responsible=true的环境变量 K8S侧配置：K8S是否允许该Pod拉取的相关配置 当三个开关条件都能满足时，则运行kubernetes-&gt;nacos的configMap数据同步。 数据流转的流程图 初始化Config Map流程（1-2） Nacos调用kubernetes Java Client的List&amp;Watch对应的方法 获取到ConfigMap数据调用PersistService持久化ConfigMap数据 实时同步Config Map流程（3-4） 此时Nacos已经Watch ConfigMap 当发生ConfigMap相关的变化时，回调ResourceEventHandlerImpl中对应的方法 然后调用PersistService持久化ConfigMap 简单代码实现config模块新建com.alibaba.nacos.config.server.service.kubernetes包，新建KubernetesConfigMapSyncService类。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283@Componentpublic class KubernetesConfigMapSyncService { private PersistService persistService; private ApiClient apiClient; private boolean isRunning; public KubernetesConfigMapSyncService(PersistService persistService) { this.persistService = persistService; boolean enable = Boolean.parseBoolean(System.getProperty(&quot;nacos.k8s.configMap.enable&quot;)); // If sync is turned on, continue execution. if (!enable) { // print log return; } // Calculate whether you are responsible. enable = true; // If the current node is responsible, it will continue to execute. if (!enable){ // print log return; } try { this.apiClient = ClientBuilder.cluster().build(); } catch (IOException e) { // print log isRunning = false; } if (enable){ watchConfigMap(); isRunning = true; } } private void watchConfigMap() { SharedInformerFactory factory = new SharedInformerFactory(apiClient); CoreV1Api coreV1Api = new CoreV1Api(); SharedIndexInformer&lt;V1ConfigMap&gt; nodeInformer = factory.sharedIndexInformerFor( (CallGeneratorParams params) -&gt; { // **NOTE**: // The following &quot;CallGeneratorParams&quot; lambda merely generates a stateless // HTTPs requests, the effective apiClient is the one specified when constructing // the informer-factory. return coreV1Api.listConfigMapForAllNamespacesCall(true, null, null, null, null, null, null, null, params.timeoutSeconds, params.watch, null); }, V1ConfigMap.class, V1ConfigMapList.class); nodeInformer.addEventHandler(new ResourceEventHandler&lt;V1ConfigMap&gt;() { @Override public void onAdd(V1ConfigMap obj) { // print log persistService.addConfigInfo4(...); } @Override public void onUpdate(V1ConfigMap oldObj, V1ConfigMap newObj) { // print log String content = convertToYaml(newObj); persistService.updateConfigInfo4(...); // todo 修改 } @Override public void onDelete(V1ConfigMap obj, boolean deletedFinalStateUnknown) { // print log persistService.removeConfigInfo(...); } private String convertToYaml(Object kubernetesResource){ return Yaml.dump(kubernetesResource); } }); // Synchronize initialization data to config } public boolean isRunning() { return isRunning; }}","link":"/2023/01/02/3_Kubernetes%E4%B8%8ENacos%E5%90%8C%E6%AD%A5ConfigMap%E6%96%B9%E6%A1%88/"},{"title":"2023年新的开始","text":"… 2023年新年快乐!即使年年不见，也要岁岁平安。","link":"/2023/01/01/1_2023%E5%B9%B4%E6%96%B0%E7%9A%84%E5%BC%80%E5%A7%8B/"},{"title":"新的一年新的里程","text":"… 2023年开始啦，打开keep的跑步记录，不知不觉已经坚持一年了，我会继续坚持。","link":"/2023/01/01/2_%E6%96%B0%E7%9A%84%E4%B8%80%E5%B9%B4%E6%96%B0%E7%9A%84%E9%87%8C%E7%A8%8B/"},{"title":"基于TUN&amp;TAP实现多个局域网设备之间的通讯","text":"emmmm… 目标本文会对tun/tap的原理进行基本说明，然后基于tun/tap实现多个局域网设备之间的通讯，实现跨网络通信。 例如：实现局域网A中的主机和局域网B中的主机进行通信。 原理简要说明物理网卡的工作流程在了解tun/tap之前，先简要说明物理网卡的工作流程。 网卡处于网络协议栈和物理网卡之间，一端连着网络协议栈，一端连着物理网络 当发送数据时：将接收网络协议栈的数据，并将数据封装成帧，并通过网线（对无线网络来说就是电磁波）将数据发送到网络中。 当接收数据时：接收网络上其他设备传过来的帧，并将帧进行解码，然后传递到网络协议栈中。 TUN、TAP基本原理目前主流的虚拟网卡方案有tun/tap和veth两种，在时间上tun/tap出现得更早，它是一组通用的虚拟驱动程序包，里面包含了两个设备，分别是用于网络数据包处理的虚拟网卡驱动，以及用于内核空间与用户空间交互的字符设备（Character Devices，这里具体指/dev/net/tun）驱动。大概在 2000 年左右，Solaris系统为了实现隧道协议（Tunneling Protocol）开发了这套驱动，从 Linux Kernel 2.1 版开始移植到Linux内核中，当时是源码中的可选模块，2.4 版之后发布的内核都会默认编译 tun/tap 的驱动。 tun 和 tap 是两个相对独立的虚拟网络设备，其中 tap 模拟了以太网设备，具备操作二层数据包（以太帧）的能力，tun 则模拟了网络层设备，具备操作三层数据包（IP 报文）的能力。使用 tun/tap 设备的目的是实现把来自协议栈的数据包先交由某个打开了/dev/net/tun字符设备的用户进程处理后，再把数据包重新发回到链路中，你可以通俗地将它理解为这块虚拟化网卡一端连接着网络协议栈，另一端连接着用户态程序，只要协议栈中的数据包能被用户态程序截获并加工处理，程序员就有足够的舞台空间去玩出各种花样，譬如数据压缩、流量加密、透明代理等功能都能够以此为基础来实现。 就以我们此次要实现的应用程序为例，当程序发送给tun设备数据包时，整个工作流程如图所示： 实现思路要实现局域网A中的主机和局域网B中的主机进行通信，我们需要有三端，分别为客户端A、客户端B、服务端C，其中服务端C部署在公有云上，它和客户端A、和客户端B的网络是通的。 分别启动服务端C客户端A、B，并且服务端C分别和客户端A、B建立TCP链接。 客户端A创建tun网卡，再对这个网卡配置IP以及子网掩码并且激活tun网卡，那么Linux会自动加上这个网段路由规则。 客户端B创建tun网卡，再对这个网卡配置IP以及子网掩码并且激活tun网卡，那么Linux会自动加上这个网段路由规则。 客户端A、B设置IP，需要在同一个网段，并且不与已有的网络冲突，例如：10.10.10.1/24、10.10.10.2/24。 客户端A向客户端B的IP（10.10.10.2）发起请求，通过路由规则的计算，将会路由到tun网卡，然后通过用户程序对tun网卡进行监听，将tun网卡的数据通过客户端A与服务端C建立的TCP连接发送出去（这里走的就是物理网卡）。 当C端接收到请求数据之后，向所有和它建立TCP链接的客户端发起广播请求（除了A端这个最初的发送端除外），如果是目的地IP，将会响应给服务端，否则将会丢弃，当服务端收到响应之后，再同理的将响应数据广播给最初的发送端。 在服务端和客户端的TCP交互过程中，我们要注意处理TCP的粘包问题。 我们前期设置tun网卡的IP和激活tun网卡，可以先手动操作，需要注意的一点就是，tun网卡它是通过用户程序进行创建的，将用户程序结束之后，tun网卡将会自动删除，所以我们需要每次维护它的IP并且手动激活它。 至此，这个程序的大致实现思路就结束了。 这里就没代码的具体实现了，主要讲讲思路，如果对代码有兴趣的话。easy-tun，注释比较细，应该很容易能够看懂。 测试这里的测试主要是对easy-tun的代码进行测试。 前提GO环境 启动服务端123456# 设置go拉取依赖的地址root@vultr:~/goWorkspace# go env -w GOPROXY=https://goproxy.cn,direct# 拉取依赖root@vultr:~/goWorkspace# go get# 启动服务端root@vultr:~/goWorkspace# go run Server.go 启动客户端并且设置IP 启动客户端A 12345678910111213# 设置go拉取依赖的地址root@vultr:~/goWorkspace# go env -w GOPROXY=https://goproxy.cn,direct# 拉取依赖root@vultr:~/goWorkspace# go get# 通过-ser指定服务端的IP地址，这里填你服务端的IP[root@localhost goworkspace]# go run Client.go -ser xxx.xxx.xxx.xxxserver address :xxx.xxx.xxx.xxxlocal tun device name :gtunconnect server succeed.# 对指定网卡设置IP，gtun为你网卡的名称，如果你的tun网卡名称不是gtun，自行调整[root@localhost goworkspace]# sudo ip addr add 10.10.10.1/24 dev gtun# 将gtun网卡激活,如果你的tun网卡名称不是gtun，自行调整[root@localhost goworkspace]# sudo ip link set gtun up 启动客户端B 12345678910111213# 设置go拉取依赖的地址root@vultr:~/goWorkspace# go env -w GOPROXY=https://goproxy.cn,direct# 拉取依赖root@vultr:~/goWorkspace# go get# 通过-ser指定服务端的IP地址，这里填你服务端的IP[root@localhost goworkspace]# go run Client.go -ser xxx.xxx.xxx.xxxserver address :xxx.xxx.xxx.xxxlocal tun device name :gtunconnect server succeed.# 对指定网卡设置IP，gtun为你网卡的名称，如果你的tun网卡名称不是gtun，自行调整[root@localhost goworkspace]# sudo ip addr add 10.10.10.2/24 dev gtun# 将gtun网卡激活,如果你的tun网卡名称不是gtun，自行调整[root@localhost goworkspace]# sudo ip link set gtun up 在客户端Aping客户端B12345678[root@localhost goworkspace]# ping 10.10.10.2PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data.64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=355 ms64 bytes from 10.10.10.2: icmp_seq=2 ttl=64 time=671 ms64 bytes from 10.10.10.2: icmp_seq=3 ttl=64 time=363 ms64 bytes from 10.10.10.2: icmp_seq=4 ttl=64 time=356 ms64 bytes from 10.10.10.2: icmp_seq=5 ttl=64 time=1473 ms64 bytes from 10.10.10.2: icmp_seq=6 ttl=64 time=592 ms 至此，我们实现了不在同一个局域网的两台机器，像访问局域网一样进行通讯。 参考 &lt;&lt;凤凰架构&gt;&gt;","link":"/2023/01/03/4_%E5%9F%BA%E4%BA%8ETUN&TAP%E5%AE%9E%E7%8E%B0%E5%A4%9A%E4%B8%AA%E5%B1%80%E5%9F%9F%E7%BD%91%E8%AE%BE%E5%A4%87%E4%B9%8B%E9%97%B4%E7%9A%84%E9%80%9A%E8%AE%AF/"},{"title":"Nacos支持监听namespace下的所有配置","text":"加油，加油！！！ 背景现有功能目前最新版本Nacos只能支持监听单个配置，通过namespace、group、dataId唯一确定一个配置文件进行监听。 新特性Nacos需要类型于kubernetes通过namespace监听此namespace下的所有单类资源的功能。也就是说Nacos需要通过namespace监听所有的配置项，当此namespace下的配置项发生变更时，通知SDK。 改造点 实现AbstractListener，定义一个NamespaceConfigChangeListener，用于namespace下的配置发生变化时进行回调。 抽象出AbstractCacheData。 基于AbstractCacheData实现SingleCacheData，单个SingleCacheData对应单个配置文件，作用于单个监听配置文件。 基于AbstractCacheData实现BatchCacheData，单个BatchCacheData对应多个匹配文件，作用于批量监听配置文件。 ConfigService&amp;ClientWorker新增addBatchListener方法，并且NacosConfigService对ConfigService的addBatchListener方法进行实现，然后修改之前所有监听单配置的方式。例如： NacosConfigService#addListener：向ClientWorker#cacheMap新增元素，元素类型为SingleCacheData，然后通知配置监听。 NacosConfigService#addBatchListener：向ClientWorker#cacheMap新增元素，元素类型为BatchCacheData，然后通知配置监听。 单个配置的监听与批量配置的监听都用ConfigBatchListenRequest进行复用。 ConfigBatchListenRequest新增configBatchListenContext用于存储批量监听的上下文。 12345678910111213public class ConfigBatchListenRequest extends AbstractConfigRequest { // 用于单个监听 private List&lt;ConfigListenContext&gt; configListenContexts = new ArrayList&lt;&gt;(); // 用户批量监听 private List&lt;ConfigBatchListenContext&gt; configBatchListenContexts = new ArrayList&lt;&gt;(); // 批量监听上下文 public static class ConfigBatchListenContext { // namespace String tenant; // &lt;groupKey, md5&gt; Map&lt;String,String&gt; md5ByGroupKeyMaps; }} ClientWorker#buildConfigRequest构建request时，对CacheData进行类型判断构建ConfigBatchListenRequest（分别填充configListenContexts和configBatchListenContexts属性）。 ConfigChangeBatchListenRequestHandler#handle分别对ConfigBatchListenRequest#configBatchListenContext与ConfigBatchListenRequest#configListenContexts进行处理。 ConfigChangeListenContext新增tenantIdContext。 1234public class ConfigChangeListenContext { // 存储tenant与connectId的映射，用于配置改变时通知指定sdk private ConcurrentHashMap&lt;String, HashSet&lt;String&gt;&gt; tenantIdContext = new ConcurrentHashMap&lt;&gt;();} ConfigCacheService新增CACHE_BY_TENANT属性，新增batchGetUpdatedDataKey方法。 12345678910public class ConfigCacheService { // 用于存储tenant与CacheItem的映射关系，发布配置时维护进去，后续可以直接通过tenant获取CacheItem private static final ConcurrentHashMap&lt;String, List&lt;CacheItem&gt;&gt; CACHE_BY_TENANT = new ConcurrentHashMap&lt;&gt;(); public static List&lt;String&gt; batchGetUpdatedDataKey(String groupKey, Map&lt;String, String&gt; md5Map) { // 从CACHE_BY_TENANT获取此tenantId对应的CacheItem，并且过滤已经变化的配置，解析成groupKey格式进行返回 return null; } } SDK端处理ConfigBatchListenRequest的响应，获取响应的配置上下文依次进行处理，分别以单个监听配置的响应和批量监听配置的响应依次进行处理。 单个listenConfig的响应： 通过dataId、group、tenant获取最新的配置，更新ClientWork#caheMap 通知listener 批量listenConfig的响应： 通过响应获取最新的配置更新到BatchCacheData中，并且通知NamespacePropertiesListener 改造后业务流程","link":"/2023/01/04/5_Nacos%E6%94%AF%E6%8C%81%E7%9B%91%E5%90%ACNamespace%E4%B8%8B%E7%9A%84%E6%89%80%E6%9C%89%E9%85%8D%E7%BD%AE/"},{"title":"Kubernetes集成NFS","text":"加油，加油！！！ NFS介绍NFS是Network File System的缩写，也就是网络文件系统，由Sun公司开发，于1984年向外公布。 它允许网络中的计算机之间共享资源，通过网络可以远程读写NFS服务器上的文件，就像访问本地文件一般。 集成部署NFS服务123456789101112131415161718# 安装$ yum -y install rpcbind nfs-utils# 创建共享目录，并且赋予权限$ mkdir /nfs -p &amp;&amp; chmod -R 777 /nfs# 编辑NFS配置，配置运行访问nfs的网段（这里允许所有IP进行访问）# rw:访问到此目录的服务器都具备读写权限# sync:数据同步写入内存和硬盘# no_all_squash：所有用户对根目录具备完全管理访问权限# no_subtree_check：不检查父目录的权限$ echo &quot;/nfs *(rw,sync,no_all_squash,no_subtree_check)&quot; &gt;&gt; /etc/exports # 载入配置$ exportfs -rv# 启动$ systemctl start rpcbind nfs# 通过showmount命令查看NFS共享情况$ showmount -e 192.168.0.211Export list for 192.168.0.211:/nfs * Node节点安装nfs-utils1$ yum -y install rpcbind nfs-utils Node节点查看NFS的共享情况1234# 通过showmount命令查看NFS共享情况$ showmount -e 192.168.0.211Export list for 192.168.0.211:/nfs * 部署RBAC12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758$ vim rbac.yamlkind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: nfs-client-provisioner-runnerrules:- apiGroups: [&quot;&quot;] resources: [&quot;persistentvolumes&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]- apiGroups: [&quot;&quot;] resources: [&quot;persistentvolumeclaims&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]- apiGroups: [&quot;&quot;] resources: [&quot;endpoints&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]- apiGroups: [&quot;storage.k8s.io&quot;] resources: [&quot;storageclasses&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]- apiGroups: [&quot;&quot;] resources: [&quot;events&quot;] verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-client-provisionersubjects:- kind: ServiceAccount name: nfs-client-provisioner namespace: defaultroleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: leader-locking-nfs-client-provisionerrules:- apiGroups: [&quot;&quot;] resources: [&quot;endpoints&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]---kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: leader-locking-nfs-client-provisionersubjects:- kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: defaultroleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io$ kubectl create -f rbac.yaml 部署Deployment123456789101112131415161718192021222324252627282930313233343536373839404142$ vim deployment.yamlapiVersion: v1kind: ServiceAccountmetadata: name: nfs-client-provisioner---kind: DeploymentapiVersion: apps/v1metadata: name: nfs-client-provisionerspec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccount: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: 192.168.0.211 - name: NFS_PATH value: /nfs volumes: - name: nfs-client-root nfs: server: 192.168.0.211 path: /nfs$ kubectl create -f deployment.yaml 部署nfs-client-provisioner并且指定NFS服务器地址，并且绑定NFS路径。 nfs-client-provisioner是一个Kubernetes的简易NFS的外部provisioner，本身不提供NFS，需要现有的NFS服务器提供存储，利用NFS Server给Kubernetes作为持久存储的后端。 部署StorageClass1234567891011$ vim class.yamlapiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: managed-nfs-storageprovisioner: fuseim.pri/ifsparameters: archiveOnDelete: &quot;false&quot;$ kubectl create -f class.yaml# 设置默认的storageClass为managed-nfs-storage$ kubectl patch storageclass managed-nfs-storage -p '{&quot;metadata&quot;: {&quot;annotations&quot;:{&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;false&quot;}}}' 创建StorageClass绑定NFS存储方式，并且设置此StorageClass为默认。 创建PVC123456789101112$ vim nfs-sc-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: nfs-sc-pvcspec: storageClassName: managed-nfs-storage # 不指定则使用默认的SC，直接使用 accessModes: - ReadWriteOnce resources: requests: storage: 1Gi PVC直接绑定StorageClass，它会自动创建对应的PV。 创建Pod123456789101112131415161718192021$ vim nfs-sc-pod.yamlkind: PodapiVersion: v1metadata: name: nfs-sc-podspec: containers: - name: test-pod image: nginx ports: - name: web containerPort: 80 volumeMounts: - name: nfs mountPath: &quot;/usr/share/nginx/html&quot; restartPolicy: &quot;Never&quot; volumes: - name: nfs persistentVolumeClaim: claimName: nfs-sc-pvc$ kubectl apply -f nfs-sc-pod.yaml 部署一个运行Nginx容器的Pod，并且暴漏80端口，将nginx的html目录挂载到NFS服务器上。 测试NFS服务器端写入文件1$ echo &quot;hello world&quot; &gt; index.html 访问NFS服务器端写入的html123456$ kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnfs-client-provisioner-74d569d687-wkp88 1/1 Running 0 5h50m 10.244.2.58 k8s-node2 &lt;none&gt; &lt;none&gt;nfs-sc-pod 1/1 Running 0 3m18s 10.244.1.23 k8s-node1 &lt;none&gt; &lt;none&gt;$ curl 10.244.1.23hello world 实验成功~","link":"/2023/01/07/6_Kubernetes%E9%9B%86%E6%88%90NFS/"},{"title":"CentOS 7设置静态IP","text":"CentOS 7设置静态IP操作记录 修改/etc/sysconfig/network-scripts/ifcfg-ens33文件，参考如下: 12345678910111213141516171819TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=static # 设置静态IP策略DEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=ens33UUID=721209c9-b80e-4840-acaf-1be104332321DEVICE=ens33ONBOOT=yes # 激活网卡IPADDR=192.168.0.211 # 设置静态IPGATEWAY=192.168.0.1 # 设置网关DNS1=8.8.8.8 # 设置DNSNETMASK=255.255.255.0 # 设置zi","link":"/2023/01/03/CentOS%207%E8%AE%BE%E7%BD%AE%E9%9D%99%E6%80%81IP/"},{"title":"Docker与Harbor","text":"Docker与Harbor笔记 DockerTODO Docker整体架构图 TODO Docker整体交互图 Dockerfile描述Dockerfile是Docker镜像的描述文件，Docker内部包含了一条条指令，每一条指令构建一层，因此每一层指令的内容，就是描述该层该如何构建。 命令EXPOSE该命令告诉容器监听连接的端口。 只有容器监听了端口，通过-P参数向外部暴漏的端口才真正生效!!! Docker安装1、安装须知安装Docker内核建议3.10版本以上。 查看Linux内核 12[root@localhost ~]# uname -r3.10.0-1160.el7.x86_64 2、更新yum包1[root@localhost ~]# yum -y update 3、卸载旧版本Docker(如果之前安装过)1[root@localhost ~]# yum remove docker docker-common docker-selinux docker-engine 4、安装Docker详细步骤4.1、安装需要的软件包 yum-util 提供yum-config-manager功能 device-mapper-persistent-data、lvm2是devicemapper驱动的依赖 1[root@localhost ~]# yum install -y yum-utils device-mapper-persistent-data lvm2 4.2、设置yum源 中央仓库源 阿里仓库源 123[root@localhost ~]# yum-config-manager --add-repo http://download.docker.com/linux/centos/docker-ce.repo[root@localhost ~]# yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 4.3、查询docker版本并安装 查询docker版本 123456789101112[root@localhost ~]# yum list docker-ce --showduplicates | sort -r已加载插件：fastestmirror可安装的软件包 * updates: mirrors.ustc.edu.cnLoading mirror speeds from cached hostfile * extras: mirrors.ustc.edu.cndocker-ce.x86_64 3:20.10.9-3.el7 docker-ce-stabledocker-ce.x86_64 3:20.10.8-3.el7 docker-ce-stabledocker-ce.x86_64 3:20.10.7-3.el7 docker-ce-stabledocker-ce.x86_64 3:20.10.6-3.el7 docker-ce-stabledocker-ce.x86_64 3:20.10.5-3.el7 docker-ce-stable... 选择一个版本并安装：yum install docker-ce-版本号 1[root@localhost ~]# yum -y install docker-ce-18.03.1.ce 4.4、启动Docker并设置Docker 开机自启12[root@localhost ~]# systemctl start docker[root@localhost ~]# systemctl enable docker Docker安装完成~~~ Docker Compose 安装1、下载docker-compose1$ sudo curl -L &quot;https://github.com/docker/compose/releases/download/v2.2.2/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose 2、将docker-compose二进制文件赋予可执行权限1$ sudo chmod +x /usr/local/bin/docker-compose 3、创建软链：1$ sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose 4、测试是否安装完成12$ docker-compose versioncker-compose version 1.24.1, build 4667896b Harbor安装教程1、下载Harbor安装包进行解压12$ wget https://github.com/goharbor/harbor/releases/download/v2.6.1/harbor-offline-installer-v2.6.1.tgz$ tar -zxvf harbor-offline-installer-v2.6.1.tgz 2、配置先复制一份harbor.yml.tmpl文件命名为harbor.yml 12# copy harbor文件$ cp harbor.yml.tmpl harbor.yml 配置harbor.yml文件，注释掉https的配置内容，配置http相关的参数，主要是hostname（本机的IP地址），port（harbor后台管理页面暴漏的端口）。 配置文件相关改动如下： 12345678910111213141516# 本机IP设置为192.168.0.203hostname: 192.168.0.203# port改为8081# http related confighttp: # port for http, default is 80. If https enabled, this port will redirect to https port port: 8081# 注释https相关内容# https related config# https: # https port for harbor, default is 443 # port: 443 # The path of cert and key files for nginx # certificate: /your/certificate/path # private_key: /your/private/key/path 注意：还有一些其他的配置，如果有需要的可以去了解一下。 例如Harbor Web端的访问地址，默认为Harbor12345，如果需要调整的话，可以自行修改相关的配置。 3、安装12$ ./prepare$ ./install.sh 4、执行docker-compose.yml，启动harbor服务1$ docker-compose up -d 5、访问Harbor地址——192.168.0.203:8081 至此安装完成啦。","link":"/2023/01/03/Docker%E4%B8%8EHarbor/"},{"title":"分布式事务解决方案之Seata","text":"加油，加油！！！ SeataSeata是什么？Seata 是一款开源的分布式事务解决方案，致力于提供高性能和简单易用的分布式事务服务。Seata 将为用户提供了 AT、TCC、SAGA 和 XA 事务模式，为用户打造一站式的分布式解决方案。 Seata术语TC - 事务协调者维护全局和分支事务的状态，驱动全局事务提交或回滚。 TM - 事务管理器定义全局事务的范围：开始全局事务、提交或回滚全局事务。 RM - 资源管理器管理分支事务处理的资源，与TC交谈以注册分支事务和报告分支事务的状态，并驱动分支事务提交或回滚。 分布式解决方案2PC即两阶段提交协议，是将整个事务流程分为两个阶段。 准备阶段：事务管理器给每个参与者发送Prepare消息，每个数据库参与者在本地执行事务，并写本地的Redo/Undo日志，此时事务没有提交。 Undo log：记录修改前的数据，用于数据库回滚。 Redo log：记录修改后的数据，用于提交事务后写入数据的文件。 提交阶段：如果事务管理器收到了参与者的执行失败或者超时消息时，直接给每个参与者发送回滚消息，否则发送提交消息，参与者根据事务管理器指令进行提交或者回滚操作。并且释放事务处理过程中使用的资源。 Seata AT模式一阶段在一阶段中，Seata会拦截”业务SQL”，首先解析SQL语义，找到要更新的业务数据，在数据更新前，保存“undo log”，然后执行“业务SQL”更新数据，更新之后保存“redo log”，最后生成锁，这些操作都是在本地数据库事务内完成，这样保证了一阶段的原子性。 二阶段如果一阶段中有本地事务没有通过，那么则执行全局回滚，否则执行全局提交，回滚用到的就是一阶段记录的“undo log”，通过回滚记录生成反向更新SQL并执行，已完成分支事务的回滚，事务完成后释放所有资源并且删除所有日志。 TCC模式实际上Seata的AT模式基本能满足我们分布式事务80%的场景，但是如果分布式事务处理如果涉及Mysql、Oracle、PostgreSQL之外的数据库以及中间件或跨语言需要手动控制整个二阶段提交过程的需求，则需要结合TCC模式，TCC模式也支持与AT模式混合使用。 一个分布式的全局事务，整体是两阶段提交（Try-[Comfire/Canel]）的模型，在Seata中，AT模式和TCC模式事实上都是两阶段提交的具体实现。TCC模式概括来说就是手工版的AT模式。 AT模式和CT模式的区别： AT模式需要依赖本地ACID事务与关系型数据库的支持 它对两个阶段的处理逻辑都是自定义的方式，所以它不依赖undo_log以及底层数据资源事务的支持，这种方式相对的也更加的有侵入性。 ATT模式的两阶段流程： 一阶段prepare：调用自定义的prepare逻辑 二阶段commit：调用自定义commit逻辑 二阶段rollback：调用自定义rollback逻辑","link":"/2023/01/09/7_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E4%B9%8BSeata/"},{"title":"Nacos","text":"Nacos源码分析 Nacos此Nacos笔记基于Nacos版本2.1.0 (Apr 29, 2022)。 编译Nacos源码 从同性交友网站上将Nacos导入IDEA 首先IDEA需要安装Protobuf Support插件（protobuf是一种数据交换格式，又称PB编码，由Google开源），安装后通过protobuf编译将proto生成java文件 在nacos目录下执行mvn -Prelease-nacos -Dmaven.test.skip=true clean install -U 在console模块中找到Nacos启动类新增VM options：-Dnacos.standalone=true单机版启动Nacos 此方式启动模式是单机版启动并且使用的是内置数据库，对于初学Nacos已经足以。 Nacos服务注册原理Nacos在2.0版本之前都是通过HTTP的方式去注册服务，在2.0版本新增Grpc的方式。 Nacos集成SpringCloudAlibaba服务注册原理入口V1版本服务注册原理——ClientV1版本服务注册原理——Serverspring-cloud-sarter-alibaba-nacos-discovery初始化Grpc连接流程spring-cloud-starter-alibaba-nacos-discovery通过com.alibaba.cloud.nacos.discovery.NacosWatch实现org.springframework.context.SmartLifecycle接口来初始化启动、停止Nacos组件。 123456789101112131415161718192021222324252627282930313233343536public class NacosWatch implements ApplicationEventPublisherAware, SmartLifecycle, DisposableBean { @Override public void start() { if (this.running.compareAndSet(false, true)) { EventListener eventListener = listenerMap.computeIfAbsent(buildKey(), event -&gt; new EventListener() { @Override public void onEvent(Event event) { if (event instanceof NamingEvent) { List&lt;Instance&gt; instances = ((NamingEvent) event) .getInstances(); Optional&lt;Instance&gt; instanceOptional = selectCurrentInstance( instances); instanceOptional.ifPresent(currentInstance -&gt; { resetIfNeeded(currentInstance); }); } } }); // 初始化NacosService NamingService namingService = nacosServiceManager .getNamingService(properties.getNacosProperties()); try { namingService.subscribe(properties.getService(), properties.getGroup(), Arrays.asList(properties.getClusterName()), eventListener); } catch (Exception e) { log.error(&quot;namingService subscribe failed, properties:{}&quot;, properties, e); } this.watchFuture = this.taskScheduler.scheduleWithFixedDelay( this::nacosServicesWatch, this.properties.getWatchDelay()); } }} NacosServiceManager是对NamingService进行管理的类，委托调用Nacos的api去创建NamingService 12345678910111213141516171819202122232425262728293031public class NacosServiceManager { public NamingService getNamingService(Properties properties) { // 如果namingService为空，则创建NamingService if (Objects.isNull(this.namingService)) { buildNamingService(properties); } return namingService; } private NamingService buildNamingService(Properties properties) { if (Objects.isNull(namingService)) { synchronized (NacosServiceManager.class) { if (Objects.isNull(namingService)) { // 创建namingService namingService = createNewNamingService(properties); } } } return namingService; } private NamingService createNewNamingService(Properties properties) { try { // 调用com.alibaba.nacos.api.NacosFactory#createNamingService(java.util.Properties)创建namingService return createNamingService(properties); } catch (NacosException e) { throw new RuntimeException(e); } }} Nacos关于Grpc的封装在Nacos2.0版本之后，Nacos支持了Grpc的通讯，如果有同学对于Grpc不了解，请先行了解Grpc。 Server整体概览： BaseRpcServer：定义了基本的服务启动以及关闭的接口。 BaseGrpcServer：实现了基本的Server模块的功能。 GrpcClusterServer：用于集群中节点的交互。 GrpcSdkServer：用于客户端和服务端的交互。 Client整体概览： RpcClient在Client端，它的整体层次和Server端是类似的，不同的是RpcServer单单定义接口，但是RpcClient不仅定义了接口，还提供了诸多的实现，例如: 消息发送 服务器列表改变，重新连接下一个服务器 …… 123456789101112131415public abstract class RpcClient implements Closeable { // 连接以及断开连接事件的阻塞队列 protected BlockingQueue&lt;ConnectionEvent&gt; eventLinkedBlockingQueue = new LinkedBlockingQueue&lt;&gt;(); // rpcClient的启动状态 protected volatile AtomicReference&lt;RpcClientStatus&gt; rpcClientStatus = new AtomicReference&lt;&gt;( RpcClientStatus.WAIT_INIT); // 重新连接信号的阻塞队列 private final BlockingQueue&lt;ReconnectContext&gt; reconnectionSignal = new ArrayBlockingQueue&lt;&gt;(1); // 服务可用列表变化，判断当前的连接的服务是否在服务可用列表中，如果不在则放入reconnectionSignal中，开始重新连接 public void onServerListChange() {...} // 将rpcClient启动状态置为STARTING // 初始化一个线程池处理eventLinkedBlockingQueue中的事件,通知对于的listener // 初始化一个线程池处理reconnectionSignal中的重新连接的信号 public final void start() throws NacosException { ... }} GrpcClient在RpcClient中定义了基本客户端与远端服务器通讯功能的抽象，而具体的通讯实现则由下面的具体实现来负责。 GrpcClient负责与远程服务器建立连接，创建一个GrpcConnection的对象，并初始化Grpc一元请求的stub以及双向流的stub，并且将他们以及初始化的Channel注入到GrpcConnection中，随后发送一个连接建立的请求，在服务端注册自己的连接。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public abstract class GrpcClient extends RpcClient { @Override public Connection connectToServer(ServerInfo serverInfo) { try { // 如果grpcExecutor为空，则初始化 if (grpcExecutor == null) { this.grpcExecutor = createGrpcExecutor(serverInfo.getServerIp()); } // 获取暴漏的端口 int port = serverInfo.getServerPort() + rpcPortOffset(); // 初始化一元请求调用的stub RequestGrpc.RequestFutureStub newChannelStubTemp = createNewChannelStub(serverInfo.getServerIp(), port); if (newChannelStubTemp != null) { // 检查stub是否有效，如果无效直接shuntDown channel Response response = serverCheck(serverInfo.getServerIp(), port, newChannelStubTemp); if (response == null || !(response instanceof ServerCheckResponse)) { shuntDownChannel((ManagedChannel) newChannelStubTemp.getChannel()); return null; } // 初始化双向流stub BiRequestStreamGrpc.BiRequestStreamStub biRequestStreamStub = BiRequestStreamGrpc .newStub(newChannelStubTemp.getChannel()); // 初始化grpcConn GrpcConnection grpcConn = new GrpcConnection(serverInfo, grpcExecutor); // 将响应的response中的connectId设置到grpcConn中 grpcConn.setConnectionId(((ServerCheckResponse) response).getConnectionId()); // create stream request and bind connection event to this connection. // 创建双向流并且将双向流绑定到grpcConn StreamObserver&lt;Payload&gt; payloadStreamObserver = bindRequestStream(biRequestStreamStub, grpcConn); // stream observer to send response to server // 设置双向流到grpcConn中 grpcConn.setPayloadStreamObserver(payloadStreamObserver); // 设置单向流到grpcConn中 grpcConn.setGrpcFutureServiceStub(newChannelStubTemp); // 设置channel到grpcConn中 grpcConn.setChannel((ManagedChannel) newChannelStubTemp.getChannel()); // send a setup request. // 向服务器发送设置双向流请求 ConnectionSetupRequest conSetupRequest = new ConnectionSetupRequest(); conSetupRequest.setClientVersion(VersionUtils.getFullClientVersion()); conSetupRequest.setLabels(super.getLabels()); conSetupRequest.setAbilities(super.clientAbilities); conSetupRequest.setTenant(super.getTenant()); grpcConn.sendRequest(conSetupRequest); // wait to register connection setup // TODO stone-98 应该是等待服务端设置 Thread.sleep(100L); return grpcConn; } return null; } catch (Exception e) { LOGGER.error(&quot;[{}]Fail to connect to server!,error={}&quot;, GrpcClient.this.getName(), e); } return null; }} GrpcConnect整体概览： Requester：定义了基本的请求接口 Connection：继承了Requester接口，在Requester接口的基础上扩展了connectionId、isAbandon字段 GrpcConnection：对Connection进行了实现 V2版本服务注册原理——ClientV2版本服务注册原理——ServerNacos相关包的作用使用Nacos分别需要导入如下两个包: 12345678&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt;&lt;/dependency&gt; spring-cloud-starter-alibaba-nacos-discovery：该项目通过自动配置以及其他 Spring 编程模型的习惯用法为 Spring Boot 应用程序在服务注册与发现方面提供和 Nacos 的无缝集成。 spring-cloud-starter-alibaba-nacos-config：Nacos 提供用于存储配置和其他元数据的 key/value 存储，为分布式系统中的外部化配置提供服务器端和客户端支持。 服务注册原理分析注册注册的入口是org.springframework.cloud.client.serviceregistry.AbstractAutoServiceRegistration#bind监听org.springframework.boot.web.context.WebServerInitializedEvent事件，当WebServer初始化完毕之后发生回调bind（）方法。 分析关键类AbstractAutoServiceRegistartion(由于篇幅原因，仅分析大概流程，不详细讲解每个方法)： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public abstract class AbstractAutoServiceRegistration&lt;R extends Registration&gt; implements AutoServiceRegistration, ApplicationContextAware { private ApplicationContext context; private Environment environment; private final ServiceRegistry&lt;R&gt; serviceRegistry; private AutoServiceRegistrationProperties properties; @EventListener(WebServerInitializedEvent.class) public void bind(WebServerInitializedEvent event) { ApplicationContext context = event.getApplicationContext(); // TODO 为什么要这样处理？ if (context instanceof ConfigurableWebServerApplicationContext) { if (&quot;management&quot;.equals( ((ConfigurableWebServerApplicationContext) context).getServerNamespace())) { return; } } this.port.compareAndSet(0, event.getWebServer().getPort()); this.start(); } public void start() { // 如果未开启服务注册，直接处理完成 if (!isEnabled()) { if (logger.isDebugEnabled()) { logger.debug(&quot;Discovery Lifecycle disabled. Not starting&quot;); } return; } // only initialize if nonSecurePort is greater than 0 and it isn't already running // because of containerPortInitializer below if (!this.running.get()) { // 依托serviceRegistry实现注册 register(); // TODO？ if (shouldRegisterManagement()) { registerManagement(); } // 自身注册之后发布事件 this.context.publishEvent(new InstanceRegisteredEvent&lt;&gt;(this, getConfiguration())); // 修改运行状态 this.running.compareAndSet(false, true); } } public void stop() {}} AbstractAutoServiceRegistration中有两个属性AutoServiceRegistrationProperties、ServiceRegistry，我们从字面基本上能猜测出，AbstractAutoServiceRegistration通过AutoServiceRegistrationProperties的属性依托ServiceRegistry从而实现自动注册，所以真正实现自动注册的应该是ServiceRegistry中。 我们回到bind()方法也是就是我们的入口，它进行上下文的namespace判断之后，初始化我们启动的端口之后调用start()方法。 start()的处理逻辑，首先判断我们是否开启服务注册，否则直接跳过，然后依托serviceRegistry完成注册之后，发布注册之后的事件。 12345678910111213141516171819202122232425262728293031323334public class NacosServiceRegistry implements ServiceRegistry&lt;Registration&gt; { ... private final NacosDiscoveryProperties nacosDiscoveryProperties; private final NamingService namingService; ... @Override public void register(Registration registration) { if (StringUtils.isEmpty(registration.getServiceId())) { log.warn(&quot;No service to register for nacos client...&quot;); return; } String serviceId = registration.getServiceId(); Instance instance = new Instance(); instance.setIp(registration.getHost()); instance.setPort(registration.getPort()); instance.setWeight(nacosDiscoveryProperties.getWeight()); instance.setClusterName(nacosDiscoveryProperties.getClusterName()); instance.setMetadata(registration.getMetadata()); try { namingService.registerInstance(serviceId, instance); log.info(&quot;nacos registry, {} {}:{} register finished&quot;, serviceId, instance.getIp(), instance.getPort()); } catch (Exception e) { log.error(&quot;nacos registry, {} register failed...{},&quot;, serviceId, registration.toString(), e); } }} 注意到NacosServoceRegistry中有两个属性： nacosDiscoveryProperties：nacos自动发现相关属性 namingService：用于客户端注册实例或者查询实例 1234567891011121314151617181920212223242526272829303132333435public class NacosNamingService implements NamingService { ... @Override public void registerInstance(String serviceName, Instance instance) throws NacosException { registerInstance(serviceName, Constants.DEFAULT_GROUP, instance); } ... @Override public void registerInstance(String serviceName, String groupName, Instance instance) throws NacosException { // nacos 1.0版本新增的ephemeral字段，它表示注册的实例是临时的还是持久的。 // 如果是临时的，则不会再Nacos服务端持久化存储，需要用心跳的方式保活，如果一段事件没有上报心跳，则会被服务端摘除。 // 持久化实例则会被Nacos服务端持久化，如果此时客户端已下线，这个实例也不会从客户端剔除，只会将健康状态设为不健康。 // 上面说了两种模式的不同和处理上的区别，那么Nacos为什么设计两种模式，它们是为了应对什么样的场景而存在呢？ // 对于临时实例，健康检查失败，则直接可以从列表中删除。这种特性就比较适合那些需要应对流量突增的场景，服务可以进行弹性扩容。当流量过去之后，服务停掉即可自动注销了。 // 对于持久化实例，健康检查失败，会被标记成不健康状态。它的好处是运维可以实时看到实例的健康状态，便于后续的警告、扩容等一些列措施。 // https://developer.aliyun.com/article/845113 // 如果是临时节点，则需要利用心跳信息进行保活 if (instance.isEphemeral()) { BeatInfo beatInfo = new BeatInfo(); beatInfo.setServiceName(NamingUtils.getGroupedName(serviceName, groupName)); beatInfo.setIp(instance.getIp()); beatInfo.setPort(instance.getPort()); beatInfo.setCluster(instance.getClusterName()); beatInfo.setWeight(instance.getWeight()); beatInfo.setMetadata(instance.getMetadata()); beatInfo.setScheduled(false); long instanceInterval = instance.getInstanceHeartBeatInterval(); beatInfo.setPeriod(instanceInterval == 0 ? DEFAULT_HEART_BEAT_INTERVAL : instanceInterval); // 定时发送心跳 beatReactor.addBeatInfo(NamingUtils.getGroupedName(serviceName, groupName), beatInfo); } // 服务zhu'ce serverProxy.registerService(NamingUtils.getGroupedName(serviceName, groupName), groupName, instance); }} 服务端12345678910111213141516171819202122// 实例相关的操作@RestController@RequestMapping(UtilsAndCommons.NACOS_NAMING_CONTEXT + UtilsAndCommons.NACOS_NAMING_INSTANCE_CONTEXT)public class InstanceController { // 注册实例 @CanDistro @PostMapping @Secured(action = ActionTypes.WRITE) public String register(HttpServletRequest request) throws Exception { final String namespaceId = WebUtils .optional(request, CommonParams.NAMESPACE_ID, Constants.DEFAULT_NAMESPACE_ID); final String serviceName = WebUtils.required(request, CommonParams.SERVICE_NAME); NamingUtils.checkServiceNameFormat(serviceName); // 构建实例 final Instance instance = HttpRequestInstanceBuilder.newBuilder() .setDefaultInstanceEphemeral(switchDomain.isDefaultInstanceEphemeral()).setRequest(request).build(); // 分别对应着两个版本，v1和v2（实现了通过grpc进行通讯）版本，通过配置选取对应的实现进行注册实例 getInstanceOperator().registerInstance(namespaceId, serviceName, instance); return &quot;ok&quot;; }} v1版本的实现： 12345678910@Componentpublic class InstanceOperatorServiceImpl implements InstanceOperator { @Override public void registerInstance(String namespaceId, String serviceName, Instance instance) throws NacosException { // 从v2版本的实例转换到v1版本 com.alibaba.nacos.naming.core.Instance coreInstance = parseInstance(instance); // 注册实例 serviceManager.registerInstance(namespaceId, serviceName, coreInstance); }} 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859@Componentpublic class ServiceManager implements RecordListener&lt;Service&gt; { // 实例的缓存信息 结构：Map(namespace, Map(group::serviceName, Service)). private final Map&lt;String, Map&lt;String, Service&gt;&gt; serviceMap = new ConcurrentHashMap&lt;&gt;(); // 注册实例 public void registerInstance(String namespaceId, String serviceName, Instance instance) throws NacosException { // 如果实例不存在，则初始化服务对象并把它放入缓存中 createEmptyService(namespaceId, serviceName, instance.isEphemeral()); Service service = getService(namespaceId, serviceName); checkServiceIsNull(service, namespaceId, serviceName); addInstance(namespaceId, serviceName, instance.isEphemeral(), instance); } public void createEmptyService(String namespaceId, String serviceName, boolean local) throws NacosException { createServiceIfAbsent(namespaceId, serviceName, local, null); } public void createServiceIfAbsent(String namespaceId, String serviceName, boolean local, Cluster cluster) throws NacosException { Service service = getService(namespaceId, serviceName); // 该服务未被初始化 if (service == null) { Loggers.SRV_LOG.info(&quot;creating empty service {}:{}&quot;, namespaceId, serviceName); service = new Service(); service.setName(serviceName); service.setNamespaceId(namespaceId); service.setGroupName(NamingUtils.getGroupName(serviceName)); // now validate the service. if failed, exception will be thrown service.setLastModifiedMillis(System.currentTimeMillis()); service.recalculateChecksum(); if (cluster != null) { cluster.setService(service); service.getClusterMap().put(cluster.getName(), cluster); } service.validate(); // 1、将实例放置缓存中 // 2、启动检查该服务下实例心跳的线程 // 3、给该服务新增监听器 putServiceAndInit(service); // 如果该实例不是临时实例，则需要新增服务信息到Nacos集群中 if (!local) { addOrReplaceService(service); } } } public Service getService(String namespaceId, String serviceName) { Map&lt;String, Service&gt; service = this.serviceMap.get(namespaceId); if (service == null) { return null; } return service.get(serviceName); }} 服务实例的监听机制对于Nacos的服务的实例都有对应的监听机制，当服务的实例发生变化时，例如：新增、删除实例，都触发对应的动作。 监听机制注册当服务注册时，有如下代码： 1234consistencyService .listen(KeyBuilder.buildInstanceListKey(service.getNamespaceId(), service.getName(), true), service);consistencyService .listen(KeyBuilder.buildInstanceListKey(service.getNamespaceId(), service.getName(), false), service); 此代码分别给服务注册了临时和持久服务对应的监听机制。 监听机制的触发以Distro协议为例，具体的逻辑在com.alibaba.nacos.naming.consistency.ephemeral.distro.DistroConsistencyServiceImpl中 123456789101112@DependsOn(&quot;ProtocolManager&quot;)@org.springframework.stereotype.Service(&quot;distroConsistencyService&quot;)public class DistroConsistencyServiceImpl implements EphemeralConsistencyService, DistroDataProcessor { ... private volatile Notifier notifier = new Notifier(); ... @PostConstruct public void init() { GlobalExecutor.submitDistroNotifyTask(notifier); } ...} 通过@PostConstruct触发通知任务。 服务实例变化之后的处理上述说到，当服务实例发生变化后，分别触发com.alibaba.nacos.naming.core.Service#onChange和com.alibaba.nacos.naming.core.Service#onDelete方法，这里讲解他们具体做了什么处理。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697@JsonInclude(Include.NON_NULL)public class Service extends com.alibaba.nacos.api.naming.pojo.Service implements Record, RecordListener&lt;Instances&gt; { @Override public void onChange(String key, Instances value) throws Exception { Loggers.SRV_LOG.info(&quot;[NACOS-RAFT] datum is changed, key: {}, value: {}&quot;, key, value); for (Instance instance : value.getInstanceList()) { // 校验实例不能为空 if (instance == null) { // Reject this abnormal instance list: throw new RuntimeException(&quot;got null instance &quot; + key); } // 权重的取值范围0.01&lt;权重&lt;10000 if (instance.getWeight() &gt; 10000.0D) { instance.setWeight(10000.0D); } if (instance.getWeight() &lt; 0.01D &amp;&amp; instance.getWeight() &gt; 0.0D) { instance.setWeight(0.01D); } } // 更新实例信息 updateIPs(value.getInstanceList(), KeyBuilder.matchEphemeralInstanceListKey(key)); // 重新计算checksum recalculateChecksum(); } @Override public void onDelete(String key) throws Exception { boolean isEphemeral = KeyBuilder.matchEphemeralInstanceListKey(key); for (Cluster each : clusterMap.values()) { each.updateIps(Collections.emptyList(), isEphemeral); } } public void updateIPs(Collection&lt;Instance&gt; instances, boolean ephemeral) { // 初始化Map&lt;clusterName, List&lt;Instance&gt;&gt; Map&lt;String, List&lt;Instance&gt;&gt; ipMap = new HashMap&lt;&gt;(clusterMap.size()); for (String clusterName : clusterMap.keySet()) { ipMap.put(clusterName, new ArrayList&lt;&gt;()); } // 遍历所有实例，将实例加入ipMap中 for (Instance instance : instances) { try { if (instance == null) { Loggers.SRV_LOG.error(&quot;[NACOS-DOM] received malformed ip: null&quot;); continue; } // 如果集群名称为空设置默认集群名称 if (StringUtils.isEmpty(instance.getClusterName())) { instance.setClusterName(UtilsAndCommons.DEFAULT_CLUSTER_NAME); } // 如果现有集群名称中不包含此集群名称则需要新建 if (!clusterMap.containsKey(instance.getClusterName())) { Loggers.SRV_LOG.warn( &quot;cluster: {} not found, ip: {}, will create new cluster with default configuration.&quot;, instance.getClusterName(), instance.toJson()); Cluster cluster = new Cluster(instance.getClusterName(), this); cluster.init(); getClusterMap().put(instance.getClusterName(), cluster); } ipMap.putIfAbsent(instance.getClusterName(), new LinkedList&lt;&gt;()); ipMap.get(instance.getClusterName()).add(instance); } catch (Exception e) { Loggers.SRV_LOG.error(&quot;[NACOS-DOM] failed to process ip: &quot; + instance, e); } } for (Map.Entry&lt;String, List&lt;Instance&gt;&gt; entry : ipMap.entrySet()) { //make every ip mine List&lt;Instance&gt; entryIPs = entry.getValue(); // 更新Cluster clusterMap.get(entry.getKey()).updateIps(entryIPs, ephemeral); } // 修改最近更新时间 setLastModifiedMillis(System.currentTimeMillis()); // 发布服务改变时间 getPushService().serviceChanged(this); // 执行双写服务 ApplicationUtils.getBean(DoubleWriteEventListener.class).doubleWriteToV2(this, ephemeral); StringBuilder stringBuilder = new StringBuilder(); for (Instance instance : allIPs()) { stringBuilder.append(instance.toIpAddr()).append('_').append(instance.isHealthy()).append(','); } // 打野所有实例 Loggers.EVT_LOG.info(&quot;[IP-UPDATED] namespace: {}, service: {}, ips: {}&quot;, getNamespaceId(), getName(), stringBuilder.toString()); }} 心跳机制Nacos的心跳机制和临时实例和持久实例的特性息息相关，所以我这里通过临时实例和持久化实例作为维度进行分析。 临时实例：临时实例只是临时注册在注册中心上，当服务下线或服务不可用时会被注册中心剔除，临时实例会与注册中心保持心跳，当服务端在指定时间没有接收到客户端的心跳信息，则会把实例状态置为不健康，然后在一段时间之后将它从注册中心剔除。 持久实例：永久实例会永久注册在注册中心，除非对它进行删除操作才能将它剔除，并且对于永久实例它可能并不知道注册中心的存在，不会向注册中心上报心跳，而是注册中心主动对他进行探活。 临时实例客户端心跳机制在客户端启动时，在发起服务注册的逻辑中，有如下代码: 1234567891011public class NacosNamingService implements NamingService { public void registerInstance(String serviceName, String groupName, Instance instance) throws NacosException { String groupedServiceName = NamingUtils.getGroupedName(serviceName, groupName); // 是否临时实例，临时实例则需要客户端主动推送心跳，而持久实例则通过服务端主动探测 if (instance.isEphemeral()) { BeatInfo beatInfo = this.beatReactor.buildBeatInfo(groupedServiceName, instance); this.beatReactor.addBeatInfo(groupedServiceName, beatInfo); } this.serverProxy.registerService(groupedServiceName, groupName, instance); }} 在上述代码中，如果注册的时临时实例，则客户端开启线程主动给注册中心上报心跳信息。 12345678910111213141516public void addBeatInfo(String serviceName, BeatInfo beatInfo) { LogUtils.NAMING_LOGGER.info(&quot;[BEAT] adding beat: {} to beat map.&quot;, beatInfo); // 通过服务名称、ip、port构建唯一key String key = this.buildKey(serviceName, beatInfo.getIp(), beatInfo.getPort()); // 如果已经注册该实例则先停止已经存在的实例 BeatInfo existBeat = null; if ((existBeat = (BeatInfo)this.dom2Beat.remove(key)) != null) { existBeat.setStopped(true); } this.dom2Beat.put(key, beatInfo); // 心跳线程开始调度 this.executorService.schedule(new BeatReactor.BeatTask(beatInfo), beatInfo.getPeriod(), TimeUnit.MILLISECONDS); // 设置metrics MetricsMonitor.getDom2BeatSizeMonitor().set((double)this.dom2Beat.size()); } 心跳任务代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class BeatTask implements Runnable { BeatInfo beatInfo; public BeatTask(BeatInfo beatInfo) { this.beatInfo = beatInfo; } public void run() { // 如果停止则直接结束，并且不开始下一次的调度 if (!this.beatInfo.isStopped()) { // 获取心跳间隔 long nextTime = this.beatInfo.getPeriod(); try { JsonNode result = BeatReactor.this.serverProxy.sendBeat(this.beatInfo, BeatReactor.this.lightBeatEnabled); long interval = result.get(&quot;clientBeatInterval&quot;).asLong(); boolean lightBeatEnabled = false; if (result.has(&quot;lightBeatEnabled&quot;)) { lightBeatEnabled = result.get(&quot;lightBeatEnabled&quot;).asBoolean(); } // 标记心跳已经开启 BeatReactor.this.lightBeatEnabled = lightBeatEnabled; // 如果服务端返回的间隔时间不为空，则以服务端返回的为准 if (interval &gt; 0L) { nextTime = interval; } int code = 10200; if (result.has(&quot;code&quot;)) { code = result.get(&quot;code&quot;).asInt(); } // 如果返回值==20404代表资源没找到，则开始新的一轮注册 if (code == 20404) { Instance instance = new Instance(); instance.setPort(this.beatInfo.getPort()); instance.setIp(this.beatInfo.getIp()); instance.setWeight(this.beatInfo.getWeight()); instance.setMetadata(this.beatInfo.getMetadata()); instance.setClusterName(this.beatInfo.getCluster()); instance.setServiceName(this.beatInfo.getServiceName()); instance.setInstanceId(instance.getInstanceId()); instance.setEphemeral(true); try { BeatReactor.this.serverProxy.registerService(this.beatInfo.getServiceName(), NamingUtils.getGroupName(this.beatInfo.getServiceName()), instance); } catch (Exception var10) { } } } catch (NacosException var11) { LogUtils.NAMING_LOGGER.error(&quot;[CLIENT-BEAT] failed to send beat: {}, code: {}, msg: {}&quot;, new Object[]{JacksonUtils.toJson(this.beatInfo), var11.getErrCode(), var11.getErrMsg()}); } // 开始下一次的心跳发起 BeatReactor.this.executorService.schedule(BeatReactor.this.new BeatTask(this.beatInfo), nextTime, TimeUnit.MILLISECONDS); } } } 临时实例服务端心跳机制1234567891011121314@RestController@RequestMapping(UtilsAndCommons.NACOS_NAMING_CONTEXT + UtilsAndCommons.NACOS_NAMING_INSTANCE_CONTEXT)public class InstanceController { @CanDistro @PutMapping(&quot;/beat&quot;) @Secured(action = ActionTypes.WRITE) public ObjectNode beat(HttpServletRequest request) throws Exception { ... int resultCode = getInstanceOperator() .handleBeat(namespaceId, serviceName, ip, port, clusterName, clientBeat, builder); ... return result; }} chuli 123456789101112131415161718192021222324252627282930313233@Override public int handleBeat(String namespaceId, String serviceName, String ip, int port, String cluster, RsInfo clientBeat, BeatInfoInstanceBuilder builder) throws NacosException { // 转换实体 com.alibaba.nacos.naming.core.Instance instance = serviceManager .getInstance(namespaceId, serviceName, cluster, ip, port); if (instance == null) { // 如果实例为空并且客户端发送的心跳也为空，则返回RESOURCE_NOT_FOUND if (clientBeat == null) { return NamingResponseCode.RESOURCE_NOT_FOUND; } // 如果客户端的心跳不为空，则重新注册 Loggers.SRV_LOG.warn(&quot;[CLIENT-BEAT] The instance has been removed for health mechanism, &quot; + &quot;perform data compensation operations, beat: {}, serviceName: {}&quot;, clientBeat, serviceName); instance = parseInstance(builder.setBeatInfo(clientBeat).setServiceName(serviceName).build()); serviceManager.registerInstance(namespaceId, serviceName, instance); } Service service = serviceManager.getService(namespaceId, serviceName); serviceManager.checkServiceIsNull(service, namespaceId, serviceName); // 如果客户端的心跳为空，则创建心跳 if (clientBeat == null) { clientBeat = new RsInfo(); clientBeat.setIp(ip); clientBeat.setPort(port); clientBeat.setCluster(cluster); } // 对心跳进行处理 service.processClientBeat(clientBeat); return NamingResponseCode.OK; } 处理逻辑 123456789101112131415161718192021222324252627282930313233343536public class ClientBeatProcessor implements BeatProcessor { @Override public void run() { Service service = this.service; if (Loggers.EVT_LOG.isDebugEnabled()) { Loggers.EVT_LOG.debug(&quot;[CLIENT-BEAT] processing beat: {}&quot;, rsInfo.toString()); } String ip = rsInfo.getIp(); String clusterName = rsInfo.getCluster(); int port = rsInfo.getPort(); // 通过集群名称获取此集群 Cluster cluster = service.getClusterMap().get(clusterName); List&lt;Instance&gt; instances = cluster.allIPs(true); // 对集群中所有实例进行处理 for (Instance instance : instances) { if (instance.getIp().equals(ip) &amp;&amp; instance.getPort() == port) { if (Loggers.EVT_LOG.isDebugEnabled()) { Loggers.EVT_LOG.debug(&quot;[CLIENT-BEAT] refresh beat: {}&quot;, rsInfo.toString()); } // 更新实例最后心跳时间 instance.setLastBeat(System.currentTimeMillis()); // 如果实例没有被标记，并且目前处于不健康状态，则更新实例的健康状态，并且发布实例状态改变事件 if (!instance.isMarked() &amp;&amp; !instance.isHealthy()) { instance.setHealthy(true); Loggers.EVT_LOG .info(&quot;service: {} {POS} {IP-ENABLED} valid: {}:{}@{}, region: {}, msg: client beat ok&quot;, cluster.getService().getName(), ip, port, cluster.getName(), UtilsAndCommons.LOCALHOST_SITE); // 对订阅此服务的客户端发送udp事件通知 getPushService().serviceChanged(service); } } } }} 持久化实例的检查机制持久化实例的检查机制是服务成功注册之后，然后通过定时任务类似的机制，由服务端主动向客户端发起探测。 123456789101112public class Cluster extends com.alibaba.nacos.api.naming.pojo.Cluster implements Cloneable { // 集群的初始化方法，开启健康检查任务 public void init() { if (inited) { return; } checkTask = new HealthCheckTask(this); HealthCheckReactor.scheduleCheck(checkTask); inited = true; }} 健康检查的核心逻辑 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class HealthCheckTask implements Runnable { @Override public void run() { try { // 如果使用了2.0+grpc的功能，则不进入 // If upgrade to 2.0.X stop health check with v1 if (ApplicationUtils.getBean(UpgradeJudgement.class).isUseGrpcFeatures()) { return; } if (distroMapper.responsible(cluster.getService().getName()) &amp;&amp; switchDomain .isHealthCheckEnabled(cluster.getService().getName())) { healthCheckProcessor.process(this); if (Loggers.EVT_LOG.isDebugEnabled()) { Loggers.EVT_LOG .debug(&quot;[HEALTH-CHECK] schedule health check task: {}&quot;, cluster.getService().getName()); } } } catch (Throwable e) { Loggers.SRV_LOG .error(&quot;[HEALTH-CHECK] error while process health check for {}:{}&quot;, cluster.getService().getName(), cluster.getName(), e); } finally { if (!cancelled) { HealthCheckReactor.scheduleCheck(this); // worst == 0 means never checked if (this.getCheckRtWorst() &gt; 0 &amp;&amp; switchDomain.isHealthCheckEnabled(cluster.getService().getName()) &amp;&amp; distroMapper.responsible(cluster.getService().getName())) { // TLog doesn't support float so we must convert it into long long diff = ((this.getCheckRtLast() - this.getCheckRtLastLast()) * 10000) / this.getCheckRtLastLast(); this.setCheckRtLastLast(this.getCheckRtLast()); Cluster cluster = this.getCluster(); if (Loggers.CHECK_RT.isDebugEnabled()) { Loggers.CHECK_RT.debug(&quot;{}:{}@{}-&gt;normalized: {}, worst: {}, best: {}, last: {}, diff: {}&quot;, cluster.getService().getName(), cluster.getName(), cluster.getHealthChecker().getType(), this.getCheckRtNormalized(), this.getCheckRtWorst(), this.getCheckRtBest(), this.getCheckRtLast(), diff); } } } } }} V2的健康检查机制健康检查的拦截链机制Nacos服务端在处理健康检查和心跳机制的时候是采用拦截链来执行的，拦截链内部有多个拦截器，通过获取不同的拦截器链实例，在实例内部指定具体的拦截器类型来组成一组拦截器。这里使用了拦截器模式和模板模式来组织代码。拦截器模式体现在整体拦截机制的实现，模板模式主要体现在对拦截器链的抽象实现上。 拦截链的核心类图 核心类： Interceptable：定义了该拦截链处理的对象基类 void passIntercept()：该对象没有被拦截器拦截，则执行该方法中具体的业务逻辑 void afterIntercept()：该对象在被拦截链拦截之后，则执行该方法中具体的业务逻辑 NacosNamingInterceptor：定义一个拦截器的基本功能，同时限定了传入的拦截对象类型必须为Interceptable以及Interceptable的子类 boolean isInterceptType(Class&lt;?&gt; type)：判断拦截器是否支持处理这个类型 boolean intercept(T object)：判断是否执行拦截操作 int order()：拦截器的优先级，数字越低优先级越高 NacosNamingInterceptorChain：定义了拦截器链对象应该具有的基本行为 void addInterceptor(NacosNamingInterceptor interceptor)：添加拦截器 void doInterceptor(T object)：执行拦截器 AbstractNamingInterceptorChain：抽象的拦截链，定义了拦截链的基本工作流程 public void addInterceptor(NacosNamingInterceptor interceptor)：向interceptors属性中新增拦截器 public void doInterceptor(T object)：通过interceptors对传入的Interceptable的子类执行拦截操作，拦截成功后调用com.alibaba.nacos.naming.interceptor.Interceptable#afterIntercept，否则调用com.alibaba.nacos.naming.interceptor.Interceptable#passIntercept AbstractNamingInterceptorChain具体的源码解析如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public abstract class AbstractNamingInterceptorChain&lt;T extends Interceptable&gt; implements NacosNamingInterceptorChain&lt;T&gt; { // 存储多个拦截器 private final List&lt;NacosNamingInterceptor&lt;T&gt;&gt; interceptors; // protected限制只有当前包和子类才能够对它进行初始化 protected AbstractNamingInterceptorChain(Class&lt;? extends NacosNamingInterceptor&lt;T&gt;&gt; clazz) { // 初始化拦截链 this.interceptors = new LinkedList&lt;&gt;(); // 使用SPI模式加载指定的拦截器类型 interceptors.addAll(NacosServiceLoader.load(clazz)); // 对拦截器的顺序进行排序 interceptors.sort(Comparator.comparingInt(NacosNamingInterceptor::order)); } /** * Get all interceptors. * 获取全部的拦截器 * * @return interceptors list */ protected List&lt;NacosNamingInterceptor&lt;T&gt;&gt; getInterceptors() { return interceptors; } /** * 新增拦截器 * @param interceptor interceptor */ @Override public void addInterceptor(NacosNamingInterceptor&lt;T&gt; interceptor) { interceptors.add(interceptor); interceptors.sort(Comparator.comparingInt(NacosNamingInterceptor::order)); } /** * 执行拦截器 * @param object be interceptor object */ @Override public void doInterceptor(T object) { // 因为内部的拦截器已经排序过了，所以直接遍历 for (NacosNamingInterceptor&lt;T&gt; each : interceptors) { // 若当前拦截的对象不是当前拦截器所要处理的类型则调过 if (!each.isInterceptType(object.getClass())) { continue; } // 执行拦截操作成功之后，继续执行拦截后操作 if (each.intercept(object)) { object.afterIntercept(); return; } } // 未拦截的操作 object.passIntercept(); }} 至此总结下拦截链的工作逻辑： ​ AbstractNamingInterceptorChain可以通过SPI机制或手动的方式添加具体的拦截器，然后调用AbstractNamingInterceptorChain.doInterceptor()方法并且传入Interceptable，拦截链则对传入Interceptable执行拦截。 当拦截成功则调用com.alibaba.nacos.naming.interceptor.Interceptable#afterIntercept 当未被拦截则调用com.alibaba.nacos.naming.interceptor.Interceptable#passIntercept 具体的业务逻辑则在afterIntercept和passIntercept方法中。 初始化流程12345678910public void init() { // 临时实例检查客户端的心跳 if (ephemeral) { beatCheckTask = new ClientBeatCheckTaskV2(this); HealthCheckReactor.scheduleCheck(beatCheckTask); } else { // 持久实例 healthCheckTaskV2 = new HealthCheckTaskV2(this); HealthCheckReactor.scheduleCheck(healthCheckTaskV2); }} 健康检查整体类图 在Nacos中，Task一共有多种类别，例如： NacosTask HealthCheckTask NacosHealthCheckTask BeatCheckTask … 在Nacos中一切操作皆为Task，他们都是Runnable的实现，可以传递给线程池执行，这也是高性能的一种有效方式。 NacosHealthCheckTask12345678910111213141516171819202122/** * Nacos health check task. * Nacos健康检查任务 * * @author xiweng.yy */public interface NacosHealthCheckTask extends Interceptable, Runnable { /** * Get task id. * 获取任务ID * * @return task id. */ String getTaskId(); /** * Do health check. * 去执行健康检查 */ void doHealthCheck();} 负责执行健康检查的任务是的核心基类是NacosHealthCheckTask。 继承Interceptable表示NacosHealthCheckTask的实现是拦截链可以处理的对象 继承Runnable表示NacosHealthCheckTask的实现可以作为Runnable的实现传递给线程池进行执行 NacosHealthCheckTask分别有两种具体实现： ClientBeatCheckTaskV2：处理心跳相关的逻辑 HealthCheckTaskV2：处理各种连接状态 ClientBeatCheckTaskV2ClientBeatCheckTaskV2负责处理心跳相关的逻辑。 1 HealthCheckTaskV2Nacos配置中心原理将K8s中的configMap同步到Nacos的配置中心背景社区规划将nacos做成k8s的数据管控中心。 将k8s中的configMap同步到Nacos的配置中间，是社区规划的重要一步。 拉取k8s数据的方式TODO nacos中存储configMap的方式 简单的存储展示 提供一个基于内存的存储方式 基于内存要考虑Server端数据的一致性 提供持久化的存储方式 建立新的表进行存储 服用 sdk中需要注入configMap的信息 附录Nacos的事件机制Nacos的服务注册和服务变更以及配置变更等等功能都是通过事件来进行通知的，理解Nacos的事件机制可以对Nacos的业务流程有更加深入的理解。 事件事件的抽象定义1234567891011121314151617181920212223242526272829public abstract class Event implements Serializable { private static final long serialVersionUID = -3731383194964997493L; private static final AtomicLong SEQUENCE = new AtomicLong(0); private final long sequence = SEQUENCE.getAndIncrement(); /** * 事件的序号，用于判断事件的顺序 */ public long sequence() { return sequence; } /** * 事件的作用域 */ public String scope() { return null; } /** * 是否为插件的事件 */ public boolean isPluginEvent() { return false; }} 慢事件的定义继承于Event，由于所有的事件的发布都共享一个队列，所以命名为慢事件。 1234567public abstract class SlowEvent extends Event { @Override public long sequence() { return 0; }} 订阅者单事件订阅者多事件订阅者1、集群名称在Nacos中，支持集群配置，集群是对指定微服务的一种虚拟分类，从而实现异地多活，就近调用。 在配置信息中指定集群名称，如下： 123456spring: cloud: nacos: discovery: # 北京机房集群，如不进行指定，则使用默认集群名称 cluster-name: BJ 2、标记服务下线","link":"/2022/09/18/Nacos/"},{"title":"Docker部署GitLab CI_CD","text":"Docker与GitLab CI/CD Docker部署GitLab CI/CD 前置条件：安装Docker环境 修改sshd默认端口 1vim /etc/ssh/sshd_config 将#Port 22改为Port 9998. 重启sshd服务 1systemctl restart sshd 出现异常:Bind to port 9998 on :: failed: Permission denied. 解决方案:https://blog.csdn.net/default7/article/details/103592139 启动gitlab容器 1234567891011docker run -d \\--hostname gitlab.example.com \\-p 443:443 \\-p 80:80 \\-p 22:22 \\--name gitlab \\--restart always \\-v $GITLAB_HOME/config:/etc/gitlab \\-v $GITLAB_HOME/logs:/var/log/gitlab \\-v $GITLAB_HOME/data:/var/opt/gitlab \\gitlab/gitlab-ce:latest 登入gitlab gitlab初始化密码在容器文件/etc/gitlab/initial_root_password此文件在初次安装gitlab24小时之后会是删除!!! GitLab Runner部署1234docker run -d --name gitlab-runner-docker \\--restart always -v $PWD:/etc/gitlab-runner \\-v /var/run/docker.sock:/var/run/docker.sock \\gitlab/gitlab-runner:latest http://192.168.0.212/","link":"/2023/01/03/Docker%E9%83%A8%E7%BD%B2GitLab%20CI%20CD/"},{"title":"Kubernetes","text":"Kubernetes笔记 Kubernetes架构api server：所有服务访问的统一入口 replication controller：管理副本的期望数量 scheduler：选择合适的节点进行任务分配 etcd：整个kubernetes的存储系统，一个分布式的键值存储服务 kubelet：kubelet和容器引擎进行交互，实现容器的生命周期管理 kube proxy：负责写入规则至iptables或ipvs实现服务映射访问（TODO不是十分理解！） kubectl：命令行工具 kubernetes重要组件coredns:可以为集群中的svc创建一个域名ip的对应解析 dashboard：给k8s提供一个b/s结构的访问体系 ingress controller：官方的只能实现4层代理，ingress可以实现7层代理 fedetation：提供一个可以跨集群中心多k8s的统一管理的功能 prometheus：提供k8s的集群监控能力 elk：提供k8s集群日志统一分析接入平台 pod的概念 自主性pod：不被k8s管理的pod 控制器管理的pod： 网络通讯方式 k8s的网络模型假定了所有Pod都在一个可以直接连通的扁平网络空间中，这在GCE中是现成的网络模型，k8s假定这个网络已经存在，而在私有云中搭建k8s集群，需要自己实现这个网络假设，将不同节点上的Docker容器之间的互相访问先打通，然后运行k8s。 资源清单名称空间级别 集群级别 元数据 Pod的生命周期安全认证 HTTP Token认证机制 HTTP Base认证机制 HTTPS的双向认证 鉴权RBACRBAC基于角色的访问控制，在kubernetes1.5版本引入，现已成为默认标准。它的优势如下： 覆盖集群中的资源和非资源属性 整个RBAC由几个对象完成，可以使用Kubectl和Api进行操作 可以在运行时调整 使用RBAC在启动api-server时将--authorization-mode资源设置为一个逗号分隔的列表并确保其包含RBAC。 1kube-apisever --authorization-mode=RBAC API对象RBAC中声明四种对象：Role、ClusterRole、RoleBinding、ClsterRoleBinding。 Role和ClusterRoleRBAC的Role和ClusterRol他们的权限是存粹累加的。 Role用来在某个命名空间中设置访问权限，在创建Pole时，你必须指定该Role所属的命名空间。 ClsterRole则是一个集群作用域的资源。ClusterRole可以用来: 定义对某命名空间域对象的访问权限，并将在个别命名空间内被赋予访问权限。 为命名空间作用域内的对象设置访问权限，并被授予跨所有命名空间的访问权限。 为集群作用域的资源定义访问权限 如果要在命名空间内定义角色，则应该使用Role，如果需要在集群范围定义角色， 则应该使用ClsterRole。 实践Kubernetes集群有两类用户：由kubernetes管理的账号和普通账号。 普通账号：是由与Kubernetes无关的服务进行管理的，Kubernetes并不包含用来代表普通用户账号的对象，普通用户的信息无法通过API调用添加到集群中，但是Kubernetes仍然认为能够提供由集群的证书机构签名的合法证书的用户是通过身份认证的用户。 创建一个用户只能管理dev空间 创建devuser-csr.json文件，内容如下： 12345678910111213141516171819{ &quot;CN&quot;:&quot;devuser&quot;, &quot;hosts&quot;:[ ], &quot;key&quot;:{ &quot;algo&quot;:&quot;rsa&quot;, &quot;size&quot;:2048 }, &quot;names&quot;:[ { &quot;C&quot;:&quot;CN&quot;, &quot;ST&quot;:&quot;BeiJing&quot;, &quot;L&quot;:&quot;BeiJing&quot;, &quot;O&quot;:&quot;k8s&quot;, &quot;OU&quot;:&quot;System&quot; } ]} 下载证书生成工具，放/user/local/bin路径下 123456wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64mv cfssl_linux-amd64 /usr/local/bin/cfsslwget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64mv cfssljson_linux-amd64 /usr/local/bin/cfssljsonwget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo 生成证书（在/etc/kubernetes/pki创建密钥信息，/etcc/kubernetes/pki中存储的都是密钥信息） 12## -ca指定私钥证书，-ca-key指定私钥 以及请求的文件 -bare指定输出的用户名cfssl gencert -ca=ca.crt -ca-key=ca.key -profile=kubernetes /root/devuser-csr.json | cfssljson -bare devuser 成功生成 ca.crt、ca.key、devuser.csr文件。（现在还不知道这些文件到底是干嘛的~~~~） 生成kubeconfig 1234567891011121314151617181920212223242526## 设置kubernetes的api server地址export KUBE_APISERVER=&quot;https://192.168.0.200:6443&quot;## 生成kubeconfig，并配置相关认证信息 (基本参数：证书、kubernetes apiserver、kubeconfig name)kubectl config set-cluster kubernetes \\--certificate-authority=/etc/kubernetes/pki/ca.crt \\--embed-certs=true \\--server=${KUBE_APISERVER} \\--kubeconfig=devuser.kubeconfig## 再配置相关认证信息kubectl config set-credentials devuser \\&gt; --client-certificate=/etc/kubernetes/pki/devuser.pem \\&gt; --client-key=/etc/kubernetes/pki/devuser-key.pem \\&gt; --embed-certs=true \\&gt; --kubeconfig=devuser.kubeconfig## 创建dev namespace（下面需要用到）kubectl create namespace dev## 配置kubernetes上下文kubectl config set-context kubernetes \\--cluster=kubernetes \\--user=devuser \\--namespace=dev \\--kubeconfig=devuser.kubeconfig 至此我们成功生成了devuser.kubeconfig。 kubernetes集群中创建对应rolebingding绑定clusterrole(admin是自带的)并且绑定user和namespace 1kubectl create rolebinding devuser-admin-binding --clusterrole=admin --user=devuser --namespace=dev devuser账户进行请求时，带上devuser.kubeconfig 1234567891011# 在devuser用户下创建.kube文件mkdir /home/devuser/.kube# 将devuser.kubeconfig拷贝到devuser用户的.kube路径下cp devuser.kubeconfig /home/devuser/.kube/.kubeconfig# 授权chown devuser:devuser /home/devuser/.kube/.kubeconfig # 切换到devuser用户su devuser## 配置上下文，相当于告诉kubectl，当我请求kubernetes时，使用.kubeconfig的认证信息kubectl config use-context kubernetes --kubeconfig=/home/devuser/.kube/.kubeconfig OK k8s部署dashboardServiceHelm三大概念 Chart代表Helm包。它包含在Kubernetes集群内部运行应用程序，工具或服务所需的所有资源定义。 Repository用来存放和共享Chart的地方。 Release是运行在Kubernetes集群中的Chart实例，对于一个Chart可以运行多次，每次都会生成一个新的Release和Release name。 Helm安装Charts到Kubernetes集群中，每次安装都会创建一个新的release，你可以在Helm的chart repositories中寻找新的charts。","link":"/2022/09/18/Kubernetes/"},{"title":"证书","text":"证书笔记 证书链在Alice和Bob的证书实验中，Alice的证书是采用我们创建的根证书签发的。在实际上真正的证书机构是不会使用根证书来直接签发用户证书的。 因为根证书非常重要，如果根证书的密钥泄露，会影响该根证书签发的所有用户，导致严重的安全风险。 中间证书为了避免上述提到的根证书的安全风险，衍生出中间证书的概念，证书机构采用根证书签发中间证书，然后使用中间证书来签发用户证书。 这样就算中间证书发生泄漏，影响的用户范围也小一些。 因此证书分为三种： 根证书 中间证书 用户证书 证书链上述说到证书分为三种，其中中间证书可以有多层，这样的证书的层级结构叫做证书链。 证书链的工作流程生成流程 证书机构生成自签名根证书 证书机构采用根证书的私钥对中间证书进行签名 证书机构使用中间证书对应的私钥签名用户证书 用户采用用户证书对应的私钥签名用户数据 证书使用和验证流程 采用中间证书的公钥验证用户证书的签名 采用根证书公钥验证中间证书的签名 采用用户证书的公钥来验证数据中的签名 证书链中证书关系如图所示，每一个下级证书中都有它上级证书的DN（Distinguished Name），在进行验证时每一级都会通过该DN找到上级证书，并使用上级证书中的Public key来验证本机证书的签名，如果有多个中间层级，则会重复该过程直到根证书，由于根证书已经内置在系统内部，属于系统信任的证书，所以验证到根证书时表示验证完成，这样就形成了一条从上到下的链状信任关系。 实践下面我们通过openssl来实践一个三层的证书链。 生成根证书和对应的密钥 123456789101112131415161718# openssl req -newkey rsa:2048 -nodes -keyout rootCA.key -x509 -days 365 -out rootCA.crtGenerating a 2048 bit RSA private key..................................+++..............................................................................+++writing new private key to 'rootCA.key'-----You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Country Name (2 letter code) [XX]:CNState or Province Name (full name) []:HunanLocality Name (eg, city) [Default City]:ChangshaOrganization Name (eg, company) [Default Company Ltd]:Test ROO^C 生成中间证书的私钥和CSR(Certificate Signing Request) 123456789101112131415161718192021222324252627282930313233343536373839404142434445# openssl req -newkey rsa:2048 -nodes -keyout rootCA.key -x509 -days 365 -out rootCA.crtGenerating a 2048 bit RSA private key.................+++.................................+++writing new private key to 'rootCA.key'-----You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Country Name (2 letter code) [XX]:CNState or Province Name (full name) []:HunanLocality Name (eg, city) [Default City]:ChangshaOrganization Name (eg, company) [Default Company Ltd]:Test root CAOrganizational Unit Name (eg, section) []:Common Name (eg, your name or your server's hostname) []:Test Root CAEmail Address []:670569467@qq.com# openssl req -new -nodes -keyout intermediate.key -out intermediate.csrGenerating a 2048 bit RSA private key..........................................................................+++.............................................+++writing new private key to 'intermediate.key'-----You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Country Name (2 letter code) [XX]:CNState or Province Name (full name) []:HunanLocality Name (eg, city) [Default City]:ChangshaOrganization Name (eg, company) [Default Company Ltd]:Test Intermediate CAOrganizational Unit Name (eg, section) []:Common Name (eg, your name or your server's hostname) []:Test Intermediate CAEmail Address []:670569467@qq.comPlease enter the following 'extra' attributesto be sent with your certificate requestA challenge password []:An optional company name []: 中间证书需要在证书的basicConstraints中设置CA:true标签，以标明该证书属于证书机构的证书，可以用于签发和验证用户证书。而openssl x509命令不能设置basicConstraints命令，因此我们需要采用openssl ca命令，该命令实现了一个简单的证书机构。 123456789101112131415161718192021222324252627[ ca ]default_ca = intermediate_ca[ intermediate_ca ]dir = .private_key = $dir/rootCA.keycertificate = $dir/rootCA.crtnew_certs_dir = $dir/serial = $dir/crt.srldatabase = $dir/db/indexdefault_md = sha256policy = policy_anyemail_in_dn = no[ policy_any ]domainComponent = optionalcountryName = optionalstateOrProvinceName = optionallocalityName = optionalorganizationName = optionalorganizationalUnitName = optionalcommonName = optionalemailAddress = optional[ ca_ext ]keyUsage = critical,keyCertSign,cRLSign# 注意这里设置了CA:true，表明使用该配置生成的证书是CA证书，可以用于签发用户证书basicConstraints = critical,CA:truesubjectKeyIdentifier = hashauthorityKeyIdentifier = keyid:always 由于openssl ca命令实现了一个简单的证书机构，会使用一个文本数据库来记录生成的证书，我们需要生成该数据库索引文件。 12mkdir dbtouch db/index 使用intermediateCA.conf生成中间证书。 123456789101112131415161718# openssl ca -config intermediateCA.conf -days 365 -create_serial -in intermediate.csr -out intermediate.crt -extensions ca_ext -notextUsing configuration from intermediateCA.confCheck that the request matches the signatureSignature okThe Subject's Distinguished Name is as followscountryName :PRINTABLE:'CN'stateOrProvinceName :ASN.1 12:'Hunan'localityName :ASN.1 12:'Changsha'organizationName :ASN.1 12:'Test Intermediate CA'commonName :ASN.1 12:'Test Intermediate CA'Certificate is to be certified until Oct 1 04:18:27 2023 GMT (365 days)Sign the certificate? [y/n]:y1 out of 1 certificate requests certified, commit? [y/n]yWrite out database with 1 new entriesData Base Updated 生成Alice的私钥和CSR 12345678910111213141516171819202122232425# openssl req -new -nodes -keyout Alice.key -out Alice.csrGenerating a 2048 bit RSA private key..................+++.........................................................+++writing new private key to 'Alice.key'-----You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Country Name (2 letter code) [XX]:CNState or Province Name (full name) []:HunanLocality Name (eg, city) [Default City]:ChangshaOrganization Name (eg, company) [Default Company Ltd]:LtdOrganizational Unit Name (eg, section) []:Common Name (eg, your name or your server's hostname) []:AliceEmail Address []:Please enter the following 'extra' attributesto be sent with your certificate requestA challenge password []:An optional company name []: 生成Alice的用户证书。 1234# openssl x509 -req -in Alice.csr -CA intermediate.crt -CAkey intermediate.key -CAcreateserial -out Alice.crtSignature oksubject=/C=CN/ST=Hunan/L=Changsha/O=Ltd/CN=AliceGetting CA Private Key 对Alice的用户证书进行验证，验证时需要同时指明根证书和中间证书。 123# openssl verify -CAfile rootCA.crt -untrusted intermediate.crt Alice.crtAlice.crt: OK 我们可以把根证书和中间证书的内容一起放到一个证书链文件中，然后使用该证书链文件来验证用户证书。 123# cat rootCA.crt intermediate.crt &gt; chain.crt# openssl verify -CAfile chain.crt Alice.crtAlice.crt: OK 在真实场景下，根证书自身就是可信的。我们将根证书导入到操作系统中来模拟该情况。(不同操作系统导入根证书操作不同！) 12#sudo cp rootCA.crt /etc/ca-certificates/trust-source/anchors/#sudo trust extract-compat 然后在openssl命令行中只指明中间证书，就可以验证Alice的用户证书。 12#openssl verify -CAfile intermediate.crt Alice.crtAlice.crt: OK 交叉认证两个CA的根证书所签发的用户证书之间应该怎么实现互信呢？我们来看下图这个例子： 在上图中，CA1和CA2是自签名证书，CA1颁发了User1证书，CA2颁发了User2证书，如何实现User1被CA2信任？ 图中使用CA2-SK对CA1进行重新签名，生成新的证书CA1&quot;。 由于User1用户证书是使用CA1的私钥进行签名，而CA1和CA1&quot;拥有相同公钥，所以CA1&quot;和User1形成互信关系，而CA1&quot;是由CA2颁发的，所以CA1&quot;和CA2也存在互信关系，从而实现User1到CA2的信任。 这样User1就处在了两条合法的证书链上： User 1 Certificate -&gt;CA1 Self-signed Certificate User 1 Certificate -&gt; CA1 Certificate Issued by CA2 -&gt; CA2 Self-signed Certificate 这两条链都是合法的，都可以对User1的证书进行验证。同理，也可以用CA1为CA2签发一个中间证书，使CA2颁发的用户证书也处于两条合法的证书链上。这种方式被称为交叉认证，通过交叉认证，可以为两个CA颁布的证书在两个CA之间建立互信关系。 通过这种方式还可以实现CA的根证书更新，在进行根证书更新时，CA生成一对新的秘钥对和根证书，然后用新的私钥为老的公钥签名生成一个中间证书，并用老的私钥为新的公钥签名生成一个中间证书。这样，无论是新的根证书还是老的根证书颁发的证书在更新期间都可以正常使用，以实现CA新老根证书的平滑过渡。","link":"/2023/01/03/%E8%AF%81%E4%B9%A6/"},{"title":"如何阅读源码","text":"如何看源码的经验 如何看源码？对于工作经验不是十分老练的程序猿来说，往往对看源码非常头疼，每每看一段源码时，可能不明白作者的真正意图，又或者被错综复杂的分支搅浑了头脑，我相信这是每个程序猿都有的时期。 我平常工作中对一些开源项目进行二开以及业余时间对中间件的学习，接触过或多或少的源码，有复杂的、简单的、能看懂的、不能看懂的都有，总结对于能看懂的源码无非是： 当我理解它这个功能的背景以及相关的技术背景之后，我脑海已经对此功能点有一个大致的实现方案，从而去看源码是对自身内心的实现方案的一个验证，而不是毫无头绪的瞎串，并且当我们自身内心有一个大致的实现方案之后，看到一些较为复杂的逻辑时，由于我们内心对此逻辑有一个大致的推测，我们便可将该逻辑当作一个黑盒，从而不会被错综复杂的分支搅乱了头脑。 例如我在阅读opennms通过lldp协议去生成网络拓扑图时，其实代码并不复杂，但是因为我对于lldp协议的不了解，导致我并不知道作者这样做的真正意图，后续当我了解了相关技术背景之后，一切便迎刃而解。 如何看懂源码？就算理解了相关功能背景之后，由于自身对它不是十分熟悉，所以在阅读源码时难免还是会遇到不懂的地方。 我常用的几种方式: 首先看官网，看官网，看官网，重要的事情说三次，看有没有相关功能的说明 看此git log，往往关联了相应的issue，我们可以从issue中了解作者的真正意图 可以在仓库提issue向开发者提问，说出你的猜想以及你的疑问 对于国内开发的开源项目，往往提供了钉钉群之类的交流渠道，我们也可以去这些地方提问 如果还不行的话，老哥先跳过这一段把-。-！，咱先不管他。 例如我看Dubbo 3.0的负载均衡算法时，发现这个一致性哈希算法中的实现，和我内心预期不一致，官网也没找到特别的说明，后续我时通过git log找到了对应的issue，发现Dubbo已经把强一致性哈希算法调整为有界负载的一致性哈希算法，当我了解有界负载的一致性哈希算法后，再来看代码，too easy~","link":"/2022/09/18/%E5%A6%82%E4%BD%95%E9%98%85%E8%AF%BB%E6%BA%90%E7%A0%81/"},{"title":"通过PID检索对应的容器","text":"通过PID找出对应的容器笔记 通过PID找出对应的容器首先通过pwdx查询pid的工作目录，如果返回/，则代表该进程由容器启动。 再通过 123456789101112$ cat /proc/23325/cgroup11:perf_event:/docker/3991c20d97ed61e63992f1b6b045885ce04d1fbe078d4d8d9cb8010a90bb8a0710:memory:/docker/3991c20d97ed61e63992f1b6b045885ce04d1fbe078d4d8d9cb8010a90bb8a079:hugetlb:/docker/3991c20d97ed61e63992f1b6b045885ce04d1fbe078d4d8d9cb8010a90bb8a078:blkio:/docker/3991c20d97ed61e63992f1b6b045885ce04d1fbe078d4d8d9cb8010a90bb8a077:cpuset:/docker/3991c20d97ed61e63992f1b6b045885ce04d1fbe078d4d8d9cb8010a90bb8a076:net_prio,net_cls:/docker/3991c20d97ed61e63992f1b6b045885ce04d1fbe078d4d8d9cb8010a90bb8a075:freezer:/docker/3991c20d97ed61e63992f1b6b045885ce04d1fbe078d4d8d9cb8010a90bb8a074:devices:/docker/3991c20d97ed61e63992f1b6b045885ce04d1fbe078d4d8d9cb8010a90bb8a073:cpuacct,cpu:/docker/3991c20d97ed61e63992f1b6b045885ce04d1fbe078d4d8d9cb8010a90bb8a072:pids:/docker/3991c20d97ed61e63992f1b6b045885ce04d1fbe078d4d8d9cb8010a90bb8a071:name=systemd:/docker/3991c20d97ed61e63992f1b6b045885ce04d1fbe078d4d8d9cb8010a90bb8a07 取容器ID的前12位，再通过命令 1$ docker ps | grep &lt;containId&gt; 就能找到对应的容器。","link":"/2023/01/03/%E9%80%9A%E8%BF%87PID%E6%A3%80%E7%B4%A2%E5%AF%B9%E5%BA%94%E7%9A%84%E5%AE%B9%E5%99%A8/"},{"title":"通过命令下载github上面的某一个文件","text":"通过命令下载github上面的某一个文件 以部署ingress-nginx为例，deploy.yaml的链接为： https://github.com/kubernetes/ingress-nginx/blob/controller-v1.4.0/deploy/static/provider/cloud/deploy.yaml 我们将github.com替换成raw.githubusercontent.com并且去除blob： https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.4.0/deploy/static/provider/cloud/deploy.yaml 并且执行命令: 1wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.4.0/deploy/static/provider/cloud/deploy.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.4.0/deploy/static/provider/cloud/deploy.yaml","link":"/2023/01/03/%E9%80%9A%E8%BF%87%E5%91%BD%E4%BB%A4%E4%B8%8B%E8%BD%BDgithub%E4%B8%8A%E9%9D%A2%E7%9A%84%E6%9F%90%E4%B8%80%E4%B8%AA%E6%96%87%E4%BB%B6/"}],"tags":[{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"Nacos","slug":"Nacos","link":"/tags/Nacos/"},{"name":"Kubernetes","slug":"Kubernetes","link":"/tags/Kubernetes/"},{"name":"感情","slug":"感情","link":"/tags/%E6%84%9F%E6%83%85/"},{"name":"跑步","slug":"跑步","link":"/tags/%E8%B7%91%E6%AD%A5/"},{"name":"Network","slug":"Network","link":"/tags/Network/"},{"name":"Golang","slug":"Golang","link":"/tags/Golang/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"Seata","slug":"Seata","link":"/tags/Seata/"},{"name":"GitLab","slug":"GitLab","link":"/tags/GitLab/"},{"name":"CI_CD","slug":"CI-CD","link":"/tags/CI-CD/"},{"name":"源码","slug":"源码","link":"/tags/%E6%BA%90%E7%A0%81/"},{"name":"Github","slug":"Github","link":"/tags/Github/"}],"categories":[{"name":"技术","slug":"技术","link":"/categories/%E6%8A%80%E6%9C%AF/"},{"name":"杂记","slug":"杂记","link":"/categories/%E6%9D%82%E8%AE%B0/"}],"pages":[{"title":"","text":"关于我我是一枚98年的程序员。在技术方面的话，我比较熟悉Java语言，同时对Golang有一定了解。在生活方面的话，我平常喜欢研究做菜，但是这和我做的难吃并不冲突，然后呢，我平时比较喜欢运动，不过这和我宅并不矛盾，哈哈哈，不过呢，以后我还是打算到处去去看看，不打算宅了。这个博客就是为了在无聊的时候记录一些琐事以及学习了解的一些技术。","link":"/about/index.html"}]}